\documentclass{article}

\title{How I scrape websites}
\author{Thomas Levine}

\newcommand\todo[1]{\textbf{#1}}

\begin{document}
\maketitle
\abstract{
I haven't yet found a guide to web scraping that explained most of
the things that I think should be explained. In particular, I have
a rather structured approach to writing scrapers, and I don't think
this approach is conveyed well in most guides to scraping. Thus, I
create this guide.
}
\section{Introduction}
\todo{Something general goes here.}

In section \ref{sec:structure}, I'll discuss how I structure my
scrapers in order to produce maintainable code.

In section \ref{sec:parsing}, I'll discuss how to navigate a variety
of source documents (mostly XML though) to extract structured
data. I'll also discuss how to use XPath and CSS selectors and how
to decide which one to use.
%Basically, use whichever one you know, use CSS if you're using
%jQuery and thus can't use XPath, and learn XPath if you don't
%know either.

In section \ref{sec:tools}, I'll discuss when how I decide which
languages, libraries, tools, \&c. to use for a given scraper.

In section \ref{sec:requests}, I'll discuss how to scrape websites
that use, cookies, POST requests or authentication. In handling
situations like these, I categorize my approaches into two categories:
(1) using a browser and (2) reverse-engineering website logic.

In section \ref{sec:database}, I'll discuss how to decide on a data
storage method and discuss schema designs for various data stores.

In section \ref{sec:errors}, I'll discuss various errors that come
up frequently and ways of handling such errors.

\section{Structure of a scraper}\label{sec:structure}
Dividing a program into small elements makes testing, collaboration
and iterative development easier. Some unnecessarily fancy names for
this are ``Don't repeat yourself (DRY)'', \todo{add more}.

For scrapers, natural divisions of program components occur at boundaries
between \emph{getting} data and \emph{parsing} data.
\footnote{You probably want to analyze/present the data too; that would be
another division, but I tend not to call that part ``scraping''.}

I'll go through a few scraper scenarios, from simple to complex, to
show how to I use the getting v. parsing criterion to divide scrapers
into manageable chunks.

\subsection{Scraping one file that is already downloaded}
Once you have the file on your computer, you can think of it as
just a plain text document (or some other type of document if it's
not plain text). All that's left to do is parse this text.
A proper walkthrough of this is in section \ref{sec:parsing},
but, essentially, you figure out the structure of the page in order
to find the data of interest and convert them to a simpler
representation that a computer can read more easily.

\subsection{Scraping one file that needs to be downloaded}
Let's say you don't have the file on your computer but you know
exactly how to get it.

In the simple case, you have a particular url that you need
to get it from, like maybe \url{http://thomaslevine.com}.
In this case, you can download the file in a line of code.

It gets more complicated if accessing the file involves a
more complex request. (This is discussed in section
\ref{sec:requests}.)

Either way, this is basically totally different from the parsing
and can easily be written as a separate component by a different person.

\subsection{Scraping many similar files that are already downloaded}
If you're only scraping one page and you're not me, you'll probably just
copy and paste the information from that page rather than writing
a script to do it. Because of this, most scrapers are going to involve
multiple files.

To scrape multiple files, I add some logic to repeat my scraping code
across multiple files. One way of doing this is to wrap the parsing
code in a for-loop.

\subsubsection{Identifiers}
In order to do this, you need some sort of identifier for each page,
otherwise you would be overwriting something or you would not be able
to link a particular row to a particular page.

\paragraph{Implicit identifiers}
Let's say you that you only want aggregate data and thus don't need to link
a particular page to a particular row. For example, it might be that you have
thousands of music playlist files and that you are only wondering how long
each playlist is.

A dumb approach that does not involve explicitly setting an page identifier
is just to parse through the files in arbitrary order and append each of the
results to a database or file.

With this approach, it is harder to figure out where errors are occurring
and it is harder to restart a parse if it is stopped in the middle.

\paragraph{Explicit identifiers}
A simple way of getting a less arbitrary identifier is to save the file name,
the file path or the url from which the file was downloaded.

This approach isn't unique in all cases. Two different directories could
have files with the same name. Two different computers could have files with
the same path. And one url could present two different files depending on
the request that was made and the time at which the request was made.

If the file name or url is not unique, you can generate a unique number
for each file or page. This doesn't necessarily help with the tracking of
errors, but it does allow you to link data based on the page from which
they were scraped.

\paragraph{Representation of identifiers in a datastore}
\todo{
add a column\\
nosql\\
if you're using flat files
}


\subsubsection{File-level data}
You can often find a more meaningful or more unique identifier from inside

\subsection{Scraping many similar files that need to be downloaded}
If the pages are similar, they will probably have some similar url structure

\end{document}
